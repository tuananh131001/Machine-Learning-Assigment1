{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Introduct Assignment](#1.1)\n",
    "* [Second Bullet Header](#1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H5wIZ2W9m9M"
   },
   "source": [
    "# Setup System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a> \n",
    "## <span style=\"color:red\"> 1. Sumary </span>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> 2. Hypothisis </span>\n",
    "***\n",
    "\n",
    "- High Weight likely get more positive stepsis\n",
    "- High Age likely get positive stepsis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\william\\anaconda3\\lib\\site-packages (5.6.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\william\\anaconda3\\lib\\site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\william\\anaconda3\\lib\\site-packages (from plotly) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install vecstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKpdpQbP9jhF"
   },
   "outputs": [],
   "source": [
    "\"\"\"Import basic modules.\"\"\"\n",
    "import numpy as np               # For linear algebra\n",
    "import pandas as pd              # For data manipulation\n",
    "import matplotlib.pyplot as plt  # For 2D visualization\n",
    "import seaborn as sns            \n",
    "from scipy import stats          # For statistics\n",
    " \n",
    "\"\"\"Plotly visualization.\"\"\"\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected = True) # Required to use plotly offline in jupyter notebook\n",
    "\n",
    "\"\"\"Machine learning models.\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\"\"\"Classification (evaluation) metrices.\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\\\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Ensembling\"\"\"\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from mlens.ensemble import BlendEnsemble\n",
    "from vecstack import stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classification (evaluation) metrices.\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "def bold(string):\n",
    "    return display(Markdown(f\"**{string}**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eY6YP-yn-nCI"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Paitients_Files_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "kyE84_n4-z_P",
    "outputId": "6610bb52-bacd-4186-adf7-d3cf21c1589c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"Paitients_Files_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Drop column ID and Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"PRG\",\"PL\",\"PR\",\"SK\",\"TS\",\"M11\",\"BD2\",\"Age\",\"Insurance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Column Sepssis to Sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename\n",
    "train.rename(columns={\"Sepssis\": \"Sepsis\"}, inplace=True)\n",
    "test.rename(columns={\"Sepssis\": \"Sepsis\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Sepsis to 0 (negative) and 1 (positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "train.loc[train['Sepsis'].isin(['Positive']), 'Sepsis'] = '1'\n",
    "train.loc[train['Sepsis'].isin(['Negative']), 'Sepsis'] = '0'\n",
    "train['Sepsis'] = train['Sepsis'].astype('int')\n",
    "numpy.array(train['Age'],dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[features]\n",
    "y = train[\"Sepsis\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check data of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Check corelation for dropping\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corelation = train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corelation, xticklabels = corelation.columns , yticklabels=corelation.columns,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.style as style\n",
    "import numpy as np\n",
    "mask = np.zeros_like(train.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.set_style('darkgrid')\n",
    "plt.subplots(figsize = (15,12))\n",
    "sns.heatmap(train.corr(), \n",
    "            annot=True,\n",
    "            mask = mask,\n",
    "            cmap = 'RdBu', ## in order to reverse the bar replace \"RdBu\" with \"RdBu_r\"\n",
    "            linewidths=.9, \n",
    "            linecolor='red',\n",
    "            fmt='.2g',\n",
    "            center = 0,\n",
    "            square=True)\n",
    "plt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20, pad = 40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Comment : </span>\n",
    " All corelation is under 50% so all data is fine\n",
    " \n",
    "#### Positive Correlation Features:\n",
    "- PL and Sepsis: \n",
    "\n",
    "#### Negative Correlation Features:\n",
    "- No negative features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "X.hist(figsize=(14,14), xrot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Comment: </span>\n",
    "PRG ,SK ,TS ,BD2 have the same distribute\n",
    "PL , PR ,M11 have the same distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Statistic of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepsis_summary = train.groupby(\"Insurance\")\n",
    "sepsis_summary.mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">  Comment: </span>\n",
    "<li>This train data set has 601 raw and 9 columns.</li>\n",
    "<li>only 34% patient got positive.</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> 4. Outliers detection</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#1.Create a function that removes outliers\"\"\"\n",
    "def removeOutliers(variable):\n",
    "    \"\"\"Calculates and removes outliers using IQR method.\"\"\"\n",
    "    \n",
    "    # Calculate 1st, 3rd quartiles and iqr.\n",
    "    q1, q3 = variable.quantile(0.25), variable.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Calculate lower fence and upper fence for outliers\n",
    "    lowerFence, upperFence = q1-1.5*iqr, q3+1.5*iqr   # Any values less than l_fence and greater than u_fence are outliers.\n",
    "    \n",
    "    # Observations that are outliers\n",
    "    outliers = variable[(variable<lowerFence) | (variable>upperFence)]\n",
    "    \n",
    "    # Drop obsevations that are outliers\n",
    "    filtered = variable.drop(outliers.index, axis = 0).reset_index(drop=True)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\"\"\"#2.Create another function to plot boxplot with and without outliers.\"\"\"\n",
    "def plotBoxPlot(variable,filteredVariable):\n",
    "    \"\"\"Plots Box plot of a variable with and without outliers.\n",
    "    We will also use the output of removeOutliers function as the input to this function.\n",
    "    variable = variable with outliers,\n",
    "    filteredVariable = variable without outliers\"\"\"\n",
    "    \n",
    "    # Create subplot object.\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        print_grid=False,\n",
    "    subplot_titles=(f\"{variable.name} Distribution with Outliers\", f\"{variable.name} Distribution without Outliers\"))\n",
    "    \n",
    "    # This trace plots boxplot with outliers\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x = variable,\n",
    "            name = \"\", # This removes trace 0\n",
    "            marker = dict(color=\"darkred\")\n",
    "        ),\n",
    "    row=1,col=1)\n",
    "    \n",
    "    # This trace plots boxplot without outliers\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x = filteredVariable,\n",
    "            name = \"\",\n",
    "            marker = dict(color=\"green\")\n",
    "        ),\n",
    "    row=2,col=1)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.layout.update(\n",
    "        height=800, \n",
    "        width=870,\n",
    "        showlegend=False,\n",
    "        paper_bgcolor=\"rgb(243, 243, 243)\",\n",
    "        plot_bgcolor=\"rgb(243, 243, 243)\"\n",
    "        )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.layout.xaxis2.update(title=f\"<b>{variable.name}</b>\")\n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot Age with and without outliers.\"\"\"\n",
    "plotBoxPlot(X.Age,removeOutliers(X.Age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot Weight with and without outliers.\"\"\"\n",
    "plotBoxPlot(X.M11,removeOutliers(X.M11))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([train, test], sort = False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's split the train and test data for bivariate analysis since test data has no Survived values. We need our target variable without missing values to conduct the association test with predictor variables.\"\"\"\n",
    "df_train = train.iloc[:599, :]\n",
    "df_test = train.iloc[599:, :]\n",
    "\n",
    "\"\"\"#1.Create a function that creates boxplot between categorical and numerical variables and calculates biserial correlation.\"\"\"\n",
    "def boxplotAndCorrelation(numVariable,catVariable=df_train.Sepsis):\n",
    "    \"\"\"Return boxplot between a categorical and numerical variable. Also calculates biserial correlation.\n",
    "    numVariable = a numerical variable of interest.\"\"\"\n",
    "    # Calculate point biserial correlation and p value\n",
    "    biserialCorr = stats.pointbiserialr(numVariable,catVariable)[0].round(2)\n",
    "    pValue = stats.pointbiserialr(numVariable,catVariable)[1].round(5)\n",
    "    \n",
    "    # Create subplot object.\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=1,\n",
    "        print_grid=False,\n",
    "    )\n",
    "    \n",
    "    # This trace plots boxplot of categorical variable vs numerical variable\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x = catVariable,\n",
    "            y = numVariable,\n",
    "            marker_color=\"lightseagreen\",\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    # Update layout\n",
    "    fig.layout.update(\n",
    "        height=500, \n",
    "        width=900,\n",
    "        showlegend=False,\n",
    "        title_text= f\"Association between {catVariable.name} and {numVariable.name} (corr: {biserialCorr}, p: {pValue})\",\n",
    "        paper_bgcolor=\"rgb(243, 243, 243)\",\n",
    "        plot_bgcolor=\"rgb(243, 243, 243)\"\n",
    "        )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.layout.xaxis1.update(title=f\"<b>{catVariable.name}</b>\")\n",
    "    fig.layout.yaxis1.update(title=f\"<b>{numVariable.name}</b>\")\n",
    "    return fig.show()\n",
    "\n",
    "\n",
    "\"\"\"#2.Create another function to calculate mean when grouped by categorical variable. And also plot the grouped mean.\"\"\"\n",
    "def numGroupedByCat(numVariable,catVariable=df_train.Sepsis):\n",
    "    \"\"\"Returns a barplot showing mean of numerical variable across the class of categorical variable.\"\"\"\n",
    "    \n",
    "    # Calculates mean across different classes of categorical variable\n",
    "    numGroupedByCat = numVariable.groupby(catVariable).mean().round(2)\n",
    "    \n",
    "    # Create subplot object.\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=1,\n",
    "        print_grid=False,\n",
    "    )\n",
    "    \n",
    "    # This trace plots barplot\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = numGroupedByCat.index,\n",
    "            y = numGroupedByCat,\n",
    "            text=numGroupedByCat,\n",
    "            hoverinfo=\"x+y\",\n",
    "            textposition=\"auto\",\n",
    "            textfont=dict(family=\"sans serif\",size=15)\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.layout.update(\n",
    "        height=500, \n",
    "        width=900,\n",
    "        showlegend=False,\n",
    "        title_text= f\"Mean {numVariable.name} across {catVariable.name}\",\n",
    "        paper_bgcolor=\"rgb(243, 243, 243)\",\n",
    "        plot_bgcolor=\"rgb(243, 243, 243)\"\n",
    "        )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.layout.xaxis1.update(title=f\"<b>{catVariable.name}</b>\")\n",
    "    fig.layout.yaxis1.update(title=f\"<b>Mean {numVariable.name}</b>\")\n",
    "    return fig.show()\n",
    "\n",
    "    \n",
    "\"\"\"#3.This function plots histogram of numerical variable for every class of categorical variable.\"\"\"\n",
    "def numHistByCat(numVariable,catVariable=df_train.Sepsis):\n",
    "    \"\"\"Returns numerical variable distribution across classes of categorical variable.\"\"\"\n",
    "    fig,ax = plt.subplots(1,1,figsize = (18,7))\n",
    "    font_size = 15\n",
    "    title_size = 18\n",
    "    numVariable[catVariable==1].hist(bins=50,color=\"green\", label = \"survived\", grid = False, alpha=0.5)\n",
    "    numVariable[catVariable==0].hist(bins=50,color=\"red\", label = \"died\", grid = False, alpha=0.5)\n",
    "    ax.set_yticks([])\n",
    "    ax.tick_params(axis=\"x\", labelsize=font_size)\n",
    "    ax.set_xlabel(f\"{numVariable.name}\", fontsize = font_size)\n",
    "    ax.set_title(f\"{numVariable.name} Distribution of Survivors vs Victims\", fontsize = title_size)\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "\n",
    "   \n",
    "\"\"\"#4.Create a function to calculate anova between numerical and categorical variable.\"\"\"\n",
    "def calculateAnova(numVariable, catVariable=df_train.Sepsis):\n",
    "    \"\"\"Returns f statistics and p value after anova calculation.\"\"\"\n",
    "    \n",
    "    groupNumVariableByCatVariable1 = numVariable[catVariable==1] # Group our numerical variable by categorical variable(1). Group Fair by survivors\n",
    "    groupNumVariableByCatVariable0 = numVariable[catVariable==0] # Group our numerical variable by categorical variable(0). Group Fare by victims\n",
    "    # Calculate one way anova\n",
    "    fValue, pValue = stats.f_oneway(groupNumVariableByCatVariable1, groupNumVariableByCatVariable0) # Calculate f statistics and p value\n",
    "    return f\"Anova Result between {numVariable.name} & {catVariable.name}: f=> {fValue}, p=> {pValue}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a boxplot to visualize the strength of association of Survived with Fare. Also calculate biserial correlation.\"\"\"\n",
    "boxplotAndCorrelation(df_train.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"So the mean fare of survivors should be much more (from positive correlation or boxplot interpretation) than those who died. Calculate mean fare paid by the survivors as well as by the victims.\"\"\"\n",
    "numGroupedByCat(df_train.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot histogram of survivor's vs victims fare.\"\"\"\n",
    "numHistByCat(df_train.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's perform ANOVA between Fare and Survived. One can omit this step. I perform just to show how anova is performed if there were more than two groups in our categorical variable.\"\"\"\n",
    "calculateAnova(df_train.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a boxplot to visualize the strength of association of Survived with Fare. Also calculate biserial correlation.\"\"\"\n",
    "boxplotAndCorrelation(df_train.M11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"So the mean fare of survivors should be much more (from positive correlation or boxplot interpretation) than those who died. Calculate mean fare paid by the survivors as well as by the victims.\"\"\"\n",
    "numGroupedByCat(df_train.M11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot histogram of survivor's vs victims fare.\"\"\"\n",
    "numHistByCat(df_train.M11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's perform ANOVA between Fare and Survived. One can omit this step. I perform just to show how anova is performed if there were more than two groups in our categorical variable.\"\"\"\n",
    "calculateAnova(df_train.M11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">  5. Data processing </span>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">  5. Data processing </span>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection \n",
    "from collections import Counter\n",
    "\n",
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    \n",
    "    return multiple_outliers   \n",
    "\n",
    "# detect outliers from Age, SibSp , Parch and Fare\n",
    "Outliers_to_drop = detect_outliers(X,2,[\"Age\",\"PRG\",\"M11\",\"BD2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[Outliers_to_drop] # Show the outliers rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No outliner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment: \n",
    "- There is no missing value\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print (train.info())\n",
    "print (\"*\"*40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>We may dont have missing values in our features.</Li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M11 filter Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter m11 using groupby\n",
    "train.loc[(train[\"M11\"] == 0), 'M11'] = np.nan\n",
    "train['M11'] = train.groupby('Age')['M11'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pl using groupby\n",
    "train.loc[(train[\"PL\"] == 0), 'PL'] = np.nan\n",
    "train['PL'] = train.groupby('Age')['PL'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> 5. EDA </span>\n",
    "***\n",
    "Before we dive into finding relations between independent variables and our dependent variable(Sepsis), let us create some assumptions about how the relations may turn-out among features.\n",
    "\n",
    "**Assumptions:**\n",
    "- PRG: High will get sepsis\n",
    "- M11  : Obese will likely get sepsis\n",
    "- AGE : old is likely get sepsis than young\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percent of people having sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='Age',\n",
    "            kind=\"violin\", data=train)\n",
    "plt.title('How old the people having sepsis are?', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"Age\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">  Comment: </span>\n",
    "Observe from the plot , patients get sepsis mostly more than 30\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Comment: </span>\n",
    "Begin from 31 the positive tends to dominance negative which means people > 30 get sepsis more than young people\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight vs Sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='M11',\n",
    "            kind=\"box\", data=train)\n",
    "plt.title('Weight vs Sepsis', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> Comment: </span>\n",
    "People are obese likely get sepsis\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='PRG',\n",
    "            kind=\"box\", data=train)\n",
    "plt.title('PRG vs Sepsis', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"PRG\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='PL',\n",
    "            kind=\"box\", data=train)\n",
    "plt.title('PL vs Sepsis', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"PL\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='SK',\n",
    "            kind=\"box\", data=train)\n",
    "plt.title('SK vs Sepsis', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"SK\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='TS',\n",
    "            kind=\"box\", data=train)\n",
    "plt.title('TS vs Sepsis', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"TS\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='BD2',\n",
    "            kind=\"box\", data=train)\n",
    "plt.title('BD2 vs Sepsis', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"BD2\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_impute(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Age_Null_Flag'] = i['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "        data = train.groupby(['Name_Title', 'Pclass'])['Age']\n",
    "        i['Age'] = data.transform(lambda x: x.fillna(x.mean()))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create bin categories for Age.\"\"\"\n",
    "weightGroups = [\"slim\",\"normal\",\"fat\"]\n",
    "\n",
    "\"\"\"Create range for each bin categories of Age.\"\"\"\n",
    "groupRanges = [0,30,55,90]\n",
    "\n",
    "\"\"\"Create and view categorized Age with original Age.\"\"\"\n",
    "X[\"M11Binned\"] = pd.cut(X.M11, groupRanges, labels = weightGroups)\n",
    "bold('**Age with Categorized Age:**')\n",
    "display(X[['M11', 'M11Binned']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create bin categories for Age.\"\"\"\n",
    "ageGroups = [\"infant\",\"child\",\"teenager\",\"youngAdult\",\"adult\",\"aged\"]\n",
    "\n",
    "\"\"\"Create range for each bin categories of Age.\"\"\"\n",
    "groupRanges = [0,5,12,18,35,60,81]\n",
    "\n",
    "\"\"\"Create and view categorized Age with original Age.\"\"\"\n",
    "X[\"ageBinned\"] = pd.cut(X.Age, groupRanges, labels = ageGroups)\n",
    "bold('**Age with Categorized Age:**')\n",
    "display(X[['Age', 'ageBinned']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert categorical data into numeric to feed our machine learning model.\"\"\"\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "\"\"\"Let's visualize the updated dataset that would be fed to our machine learning algorithms.\"\"\"\n",
    "bold(\"Preview of Processed Data:\")\n",
    "display(X.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Drop the features that would not be useful anymore.\"\"\"\n",
    "X.drop(columns = [\"M11\",\"Age\"], inplace = True, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> 5. Traning Machine Learning Model </span>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Pre-Modeling Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 43\n",
    "X_train , X_test ,y_train ,y_test = train_test_split(X,y , train_size = 0.8 , test_size = 0.2 , random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"See the dimensions of input and output data set.\"\"\"\n",
    "print(f\"Input Matrix Dimension: {X_train.shape}\")\n",
    "print(f\"Output Vector Dimension: {y_train.shape}\")\n",
    "print(f\"Test Data Dimension: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate model with Kfold stratified cross val\n",
    "kfold = StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Building machine learning models: \n",
    "We will try 10 different classifiers to find the best classifier after tunning model's hyperparameters that will best generalize the unseen(test) data.\"\"\"\n",
    "\n",
    "\"\"\"Now initialize all the classifiers object.\"\"\"\n",
    "\"\"\"#1.Logistic Regression\"\"\"\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\"\"\"#3.Random Forest Classifier\"\"\"\n",
    "rf = RandomForestClassifier(random_state = seed, n_estimators = 100)\n",
    "\n",
    "\"\"\"#6.Decision Tree Classifier\"\"\"\n",
    "dt = DecisionTreeClassifier(random_state = seed,ccp_alpha=0.01)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"List of all the models with their indices.\"\"\"\n",
    "modelNames = [\"LR\",\"RF\", \"DT\"]\n",
    "models = [lr,rf,dt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a function that returns train accuracy of different models.\"\"\"\n",
    "def calculateTrainAccuracy(model):\n",
    "    \"\"\"Returns training accuracy of a model.\"\"\"\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    trainAccuracy = model.score(X_train, y_train)\n",
    "    trainAccuracy = round(trainAccuracy*100, 2)\n",
    "    return trainAccuracy\n",
    "\n",
    "# Calculate train accuracy of all the models and store them in a dataframe\n",
    "modelScores = list(map(calculateTrainAccuracy, models))\n",
    "trainAccuracy = pd.DataFrame(modelScores, columns = [\"trainAccuracy\"], index=modelNames)\n",
    "trainAccuracySorted = trainAccuracy.sort_values(by=\"trainAccuracy\", ascending=False)\n",
    "bold(\"Training Accuracy of the Classifiers:\")\n",
    "display(trainAccuracySorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a function that returns mean cross validation score for different models.\"\"\"\n",
    "def calculateXValScore(model):\n",
    "    \"\"\"Returns models' cross validation scores.\"\"\"\n",
    "    \n",
    "    xValScore = cross_val_score(model, X_train, y_train, cv = 10, scoring=\"accuracy\").mean()\n",
    "    xValScore = round(xValScore*100, 2)\n",
    "    return xValScore\n",
    "\n",
    "# Calculate cross validation scores of all the models and store them in a dataframe\n",
    "modelScores = list(map(calculateXValScore, models))\n",
    "xValScores = pd.DataFrame(modelScores, columns = [\"xValScore\"], index=modelNames)\n",
    "xValScoresSorted = xValScores.sort_values(by=\"xValScore\", ascending=False)\n",
    "bold(\"Models 10-fold Cross Validation Score:\")\n",
    "display(xValScoresSorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define all the models\" hyperparameters one by one first::\"\"\"\n",
    "\n",
    "\"\"\"Define hyperparameters the logistic regression will be tuned with. For LR, the following hyperparameters are usually tunned.\"\"\"\n",
    "lrParams = {\"penalty\":[\"l1\", \"l2\"],\n",
    "            \"C\": np.logspace(0, 4, 10),\n",
    "            \"max_iter\":[5000]}\n",
    "\n",
    "\n",
    "\"\"\"For DT, the following hyperparameters are usually tunned.\"\"\"\n",
    "dtParams = {\"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "            \"max_depth\": [2,4,6],\n",
    "             \"min_samples_split\": np.arange(2,16), \n",
    "             \"min_samples_leaf\":np.arange(1,12),\n",
    "             \"random_state\":[seed]}\n",
    "\n",
    "\"\"\"For RF, the following hyperparameters are usually tunned.\"\"\"\n",
    "rfParams = {\"criterion\":[\"gini\",\"entropy\"],\n",
    "            \"max_depth\": [2,4,6],\n",
    "             \"n_estimators\":[10, 15, 20, 25, 30],\n",
    "             \"min_samples_leaf\":[1, 2, 3],\n",
    "             \"min_samples_split\":np.arange(3,8), \n",
    "             \"max_features\":[\"sqrt\", \"auto\", \"log2\"],\n",
    "             \"random_state\":[44]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a function to tune hyperparameters of the selected models.\"\"\"\n",
    "def tuneHyperparameters(model, params):\n",
    "    \"\"\"Returns best score of a model and its corresponding hyperparameters.\n",
    "    model = model to be optimized.\n",
    "    params = hyperparameters the models will be optimized with.\"\"\"\n",
    "    \n",
    "    # Construct grid search object with 10 fold cross validation.\n",
    "    gridSearch = GridSearchCV(model, params, verbose=0, cv=3, scoring=\"accuracy\", n_jobs = -1)\n",
    "    # Fit using grid search.\n",
    "    gridSearch.fit(X_train, y_train)\n",
    "    bestParams, bestScore = gridSearch.best_params_, round(gridSearch.best_score_*100, 2)\n",
    "    return bestScore, bestParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Due to computational restrictions, I won't optimise xgbc's hyperparameters.\"\"\"\n",
    "modelNamesToTune = [x for x in modelNames]\n",
    "modelsToTune = [lr, rf, dt]\n",
    "parametersLists = [lrParams, rfParams, dtParams]\n",
    "bestScoreAndHyperparameters = list(map(tuneHyperparameters, modelsToTune, parametersLists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's create a dataframe to store best score and best params.\"\"\"\n",
    "bestScoreAndHyperparameters = pd.DataFrame(bestScoreAndHyperparameters,\n",
    "                                             index=modelNamesToTune,\n",
    "                                             columns=[\"tunedAccuracy\", \"bestHyperparameters\"])\n",
    "bestScoreAndHyperparametersSorted = bestScoreAndHyperparameters.sort_values(by=\"tunedAccuracy\",\n",
    "                                                                                ascending=False)\n",
    "bold(\"Model's Accuracy after Tuning Hyperparameters:\")\n",
    "display(bestScoreAndHyperparametersSorted.iloc[:,0].to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's check out LR separately.\"\"\"\n",
    "print(f\"LR Best Score: {bestScoreAndHyperparametersSorted.loc['LR'][0]}\")\n",
    "print(f\"And Best Parameters: {bestScoreAndHyperparametersSorted.loc['LR'][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's check out LR separately.\"\"\"\n",
    "print(f\"RF Best Score: {bestScoreAndHyperparametersSorted.loc['RF'][0]}\")\n",
    "print(f\"And Best Parameters: {bestScoreAndHyperparametersSorted.loc['RF'][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's check out LR separately.\"\"\"\n",
    "print(f\"DT Best Score: {bestScoreAndHyperparametersSorted.loc['DT'][0]}\")\n",
    "print(f\"And Best Parameters: {bestScoreAndHyperparametersSorted.loc['DT'][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dt.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "print(ccp_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each alpha we will append our model to a list\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    df = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    df.fit(X_train, y_train)\n",
    "    clfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "plt.scatter(ccp_alphas,node_counts)\n",
    "plt.scatter(ccp_alphas,depth)\n",
    "plt.plot(ccp_alphas,node_counts,label='no of nodes',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,depth,label='depth',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "for c in clfs:\n",
    "    y_train_pred = c.predict(X_train)\n",
    "    y_test_pred = c.predict(X_test)\n",
    "    train_acc.append(accuracy_score(y_train_pred,y_train))\n",
    "    test_acc.append(accuracy_score(y_test_pred,y_test))\n",
    "\n",
    "plt.scatter(ccp_alphas,train_acc)\n",
    "plt.scatter(ccp_alphas,test_acc)\n",
    "plt.plot(ccp_alphas,train_acc,label='train_accuracy',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,test_acc,label='test_accuracy',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Instantiate the models with optimized hyperparameters.\"\"\"\n",
    "# Sort the dataframe by index and select bestHyperparameters column\n",
    "tunedParams = bestScoreAndHyperparametersSorted.sort_index().loc[:,\"bestHyperparameters\"]\n",
    "print(tunedParams)\n",
    "dt  = DecisionTreeClassifier(**tunedParams[\"DT\"],ccp_alpha=0.02)\n",
    "lr  = LogisticRegression(**tunedParams[\"LR\"])\n",
    "rf  = RandomForestClassifier(**tunedParams[\"RF\"])\n",
    "\n",
    "\n",
    "\"\"\"Train all the models with optimised hyperparameters.\"\"\"\n",
    "models = [ dt ,lr, rf]\n",
    "modelNames = tunedParams.index.values\n",
    "keyValue = dict(zip(modelNames, models))\n",
    "bold(\"10-fold Cross Validation after Optimization:\")\n",
    "xValScore = []\n",
    "for key, value in keyValue.items():\n",
    "    # Train the models with optimized parameters using cross validation.\n",
    "    # No need to fit the data. cross_val_score does that for us.\n",
    "    # But we need to fit train data for prediction in the follow session.\n",
    "    value.fit(X_train, y_train)\n",
    "    scores = cross_val_score(value, X_train, y_train, cv = 10, scoring=\"accuracy\")*100\n",
    "    xValScore.append(scores.mean())\n",
    "    print(\"Mean Accuracy: {:.4f} (+/- {:.4f}) [{}]\".format(scores.mean(), scores.std(), key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a function that compares cross validation scores with tunned scores for different models by\n",
    "plotting them.\"\"\"\n",
    "def compareModelsAccuracy():\n",
    "    \"\"\"Returns a stack bar chart of tuned and x validation scores of models.\"\"\"\n",
    "    \n",
    "    # Sort by index and converting to series object to plot.\n",
    "    xValScore = xValScoresSorted[~xValScoresSorted.index.isin([\"XGBC\",\"GNB\"])].sort_index().T.squeeze()\n",
    "    tunedScore = bestScoreAndHyperparametersSorted.iloc[:,0].sort_index().T.squeeze()\n",
    "    \n",
    "    # Create two subplots of stack bar chart\n",
    "    fig=make_subplots(\n",
    "        rows=1, \n",
    "        cols=1,\n",
    "        vertical_spacing=0.3,\n",
    "        print_grid=False) # This suppresses \"This is the format of your plot grid:\" text from popping out.\n",
    "\n",
    "    # Add trace for stack bar\n",
    "    fig.add_trace(go.Bar(x=xValScore.index,\n",
    "                             y=xValScore,\n",
    "                             text=xValScore,\n",
    "                             hoverinfo=\"x+y\",\n",
    "                             textposition=\"auto\",\n",
    "                             name=\"xValScore\",\n",
    "                             textfont=dict(family=\"sans serif\",size=14),\n",
    "                             ),\n",
    "                     row=1,\n",
    "                     col=1\n",
    "                     )\n",
    "\n",
    "    # Add another trace for stack bar\n",
    "    fig.add_trace(go.Bar(x=tunedScore.index,\n",
    "                             y=tunedScore,\n",
    "                             text=tunedScore,\n",
    "                             hoverinfo=\"x+y\",\n",
    "                             textposition=\"auto\",\n",
    "                             name=\"tunedScores\",\n",
    "                             textfont=dict(family=\"sans serif\",size=14),\n",
    "                             ),\n",
    "                     row=1,\n",
    "                     col=1\n",
    "                     )\n",
    "        \n",
    "    # Update the layout. Add title, dimension, and background color\n",
    "    fig.layout.update(\n",
    "        height=600, \n",
    "        width=950,\n",
    "        hovermode=\"closest\",\n",
    "        barmode = \"stack\",\n",
    "        title_text = \"Cross Vaidation Scores vs Optimized Scores\",\n",
    "        paper_bgcolor=\"rgb(243, 243, 243)\",\n",
    "        plot_bgcolor=\"rgb(243, 243, 243)\"\n",
    "        )\n",
    "\n",
    "    # Set y-axis titles in bold\n",
    "    fig.layout.yaxis1.update(title=\"<b>%Accuracy</b>\")\n",
    "    \n",
    "    # Set x-axis title in bold\n",
    "    fig.layout.xaxis1.update(title=\"<b>Models</b>\")\n",
    "    return fig.show()\n",
    "\n",
    "\"\"\"Call the function to plot the scores.\"\"\"\n",
    "compareModelsAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a function that returns learning curves for different classifiers.\"\"\"\n",
    "def plotLearningCurve(model):\n",
    "    \"\"\"Returns a plot of learning curve of a model.\"\"\"\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X, y = X_train, y_train\n",
    "    # Create CV training and test scores for various training set sizes\n",
    "    trainSizes, trainScores, testScores = learning_curve(model, X, y, cv = 10,\n",
    "                                                    scoring=\"accuracy\", n_jobs = -1, \n",
    "                                                    train_sizes = np.linspace(0.01, 1.0, 17), # 17 different sizes of the training set\n",
    "                                                    random_state = seed)\n",
    "                                                    \n",
    "\n",
    "    # Create means and standard deviations of training set scores\n",
    "    trainMean = np.mean(trainScores, axis = 1)\n",
    "    trainStd = np.std(trainScores, axis = 1)\n",
    "\n",
    "    # Create means and standard deviations of test set scores\n",
    "    testMean = np.mean(testScores, axis = 1)\n",
    "    testStd = np.std(testScores, axis = 1)\n",
    "\n",
    "    # Draw lines\n",
    "    plt.plot(trainSizes, trainMean, \"o-\", color = \"red\",  label = \"training score\")\n",
    "    plt.plot(trainSizes, testMean, \"o-\", color = \"green\", label = \"cross-validation score\")\n",
    "    \n",
    "    # Draw bands\n",
    "    plt.fill_between(trainSizes, trainMean - trainStd, trainMean + trainStd, alpha = 0.1, color = \"r\") # Alpha controls band transparency.\n",
    "    plt.fill_between(trainSizes, testMean - testStd, testMean + testStd, alpha = 0.1, color = \"g\")\n",
    "\n",
    "    # Create plot\n",
    "    font_size = 15\n",
    "    plt.xlabel(\"Training Set Size\", fontsize = font_size)\n",
    "    plt.ylabel(\"Accuracy Score\", fontsize = font_size)\n",
    "    plt.xticks(fontsize = font_size)\n",
    "    plt.yticks(fontsize = font_size)\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now plot learning curves of the optimized models in subplots.\"\"\"\n",
    "plt.figure(figsize = (25,25))\n",
    "lcModels = [rf,  dt, lr]\n",
    "lcLabels = [\"RF\", \"DT\", \"LR\"]\n",
    "\n",
    "for ax, model, label in zip (range(1,9), lcModels, lcLabels):\n",
    "    plt.subplot(3,1,ax)\n",
    "    plotLearningCurve(model)\n",
    "    plt.title(label, fontsize = 18)\n",
    "plt.suptitle(\"Learning Curves of Optimized Models\", fontsize = 28)\n",
    "plt.tight_layout(rect = [0, 0.03, 1, 0.97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Return prediction to use it in another function.\"\"\"\n",
    "def xValPredict(model):\n",
    "    \"\"\"Returns prediction by which we can calculate different classification metrices.\"\"\"\n",
    "    \n",
    "    predicted = cross_val_predict(model, X_train, y_train, cv = 10)\n",
    "    return predicted # Now we can use it in another function by assigning the function to its return value.\n",
    "\n",
    "\"\"\"Function to compute classification report.\"\"\"\n",
    "def calculateClassificationReport(model):\n",
    "    \"\"\"Returns a model\"s classification report.\"\"\"\n",
    "    \n",
    "    predicted = xValPredict(model)\n",
    "\n",
    "    classificationReport = classification_report(y_train, predicted)\n",
    "    \n",
    "    return print(classificationReport)\n",
    "\n",
    "\"\"\"Now calculate classification report for rf and gbc.\"\"\"\n",
    "bold(\"LR Classification Report:\")\n",
    "calculateClassificationReport(lr)\n",
    "bold(\"RF Classification Report:\")\n",
    "calculateClassificationReport(rf)\n",
    "bold(\"dt Classification Report:\")\n",
    "calculateClassificationReport(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction & Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Submission with the most accurate gradient boosting classifier.\"\"\"\n",
    "submissionLR = pd.DataFrame({\n",
    "        \"Sepsis\": lr.predict(X_train)})\n",
    "submissionLR.to_csv(\"lfSubmission.csv\", index = False)\n",
    "\"\"\"Submission with the most accurate random forest classifier.\"\"\"\n",
    "submissionRF = pd.DataFrame({\n",
    "\n",
    "        \"Sepsis\": rf.predict(X_train)})\n",
    "submissionRF.to_csv(\"rfSubmission.csv\", index = False)\n",
    "\n",
    "\n",
    "\"\"\"Submission with the most accurate gradient boosting classifier.\"\"\"\n",
    "submissionGBC = pd.DataFrame({\n",
    "        \"Sepsis\": dt.predict(X_train)})\n",
    "submissionGBC.to_csv(\"dtSubmission.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_pred_proba = lr.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.plot(fpr,tpr, label='Knn')\n",
    "plt.xlabel('fpr')\n",
    "plt.ylabel('tpr')\n",
    "plt.title('Knn(n_neighbors=11) ROC curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area under ROC curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test,y_preds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

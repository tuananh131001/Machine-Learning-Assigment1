{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOC:\n",
    "* [Introduct Assignment](#1.1)\n",
    "* [Second Bullet Header](#1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install vecstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Import necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKpdpQbP9jhF"
   },
   "outputs": [],
   "source": [
    "\"\"\"Import basic modules.\"\"\"\n",
    "import numpy as np               # For linear algebra\n",
    "import pandas as pd              # For data manipulation\n",
    "import matplotlib.pyplot as plt  # For 2D visualization\n",
    "import seaborn as sns            \n",
    "from scipy import stats          # For statistics\n",
    " \n",
    "\"\"\"Plotly visualization.\"\"\"\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "init_notebook_mode(connected = True) # Required to use plotly offline in jupyter notebook\n",
    "\n",
    "\"\"\"Machine learning models.\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\"\"\"Classification (evaluation) metrices.\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\\\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Ensembling\"\"\"\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from mlens.ensemble import BlendEnsemble\n",
    "from vecstack import stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classification (evaluation) metrices.\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "def bold(string):\n",
    "    return display(Markdown(f\"**{string}**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1.1\"></a> \n",
    "## <h1 style=\"color:red;font-size:40px;font-family:Georgia;text-align:center;\"><strong>Table of Content</strong></h1>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h1 style=\"color:red;font-size:40px;font-family:Georgia;text-align:center;\"><strong>1. Hypothisis </strong></h1>\n",
    "***\n",
    "\n",
    "- High Weight likely get more positive stepsis\n",
    "- High Age likely get positive stepsis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;font-size:40px;font-family:Georgia;text-align:center;\"><strong> 2. Data Preparation & Data exploration (EDA) </strong></h1>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into finding relations between independent variables and our dependent variable(Sepsis), let us create some assumptions about how the relations may turn-out among features.\n",
    "\n",
    "**Assumptions:**\n",
    "- PRG: High will get sepsis\n",
    "- M11  : Obese will likely get sepsis\n",
    "- AGE : old is likely get sepsis than young\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eY6YP-yn-nCI"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Paitients_Files_Train.csv\").drop(\"ID\", axis=1)\n",
    "train.columns = train.columns.str.replace(' ', '') #strip the extra-whitespaces out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = pd.read_csv(\"Paitients_Files_Train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"Paitients_Files_Test.csv\")\n",
    "test.columns = test.columns.str.replace(' ', '') #strip the extra-whitespaces ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "The TRAIN Dataframe contain 598 records and 10 columns. There are 598 training examples in the dataset, this is a good sign since there seems to be large enough data for machine learning. The shape of the dataset tells is that I have 10 attributes. Of the 10 attributes, one is the target variable that the model should predict. This means that I have 9 attributes that have the potential to be used to train my future predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Check data types & Make the data homogeneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns to the best possible dtypes, object->string\n",
    "train = train.convert_dtypes()\n",
    "test = test.convert_dtypes()\n",
    "validation = validation.convert_dtypes()\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "The method .info() is great for checking out the data types of the different features already coverted into the desired types and non-null values. However, it is not great for getting a visual picture of what is missing for the different features. You will use missingno for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#1.Create a function to calculate missing values\"\"\"\n",
    "def calculateMissingValues(variable):\n",
    "    \"\"\"Calculates missing values of a variable.\"\"\"\n",
    "    \n",
    "    return train.isna().sum()[train.isna().sum()>0] # Returns only columns with missing values\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\"#2.Create a function to plot scatter plot.\n",
    "This can also be used to plot missing values\"\"\"\n",
    "def plotScatterPlot(x, y, title, yaxis):\n",
    "    trace = go.Scatter(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    mode = \"markers\",\n",
    "    marker = dict(color = y, size = 35, showscale = True, colorscale = \"Rainbow\"))\n",
    "    layout = go.Layout(hovermode= \"closest\",\n",
    "                       title = title,\n",
    "                       yaxis = dict(title = yaxis),\n",
    "                       height=600,\n",
    "                       width=900,\n",
    "                       showlegend=False,\n",
    "                        paper_bgcolor=\"rgb(243, 243, 243)\",\n",
    "                        plot_bgcolor=\"rgb(243, 243, 243)\"\n",
    "                      )\n",
    "    fig = go.Figure(data = [trace], layout = layout)\n",
    "    return fig.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot variables with their corresponding missing values.\"\"\"\n",
    "plotScatterPlot(calculateMissingValues(train).index,\n",
    "               calculateMissingValues(train),\n",
    "               \"Features with Missing Values\",\n",
    "               \"Missing Values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "There is no missing values in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform to UPERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast all values inside the dataframe (except the columns' name) into upper case.\n",
    "train = train.applymap(lambda s: s.upper() if type(s) == str else s)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast all values inside the dataframe (except the columns' name) into upper case.\n",
    "test = test.applymap(lambda s: s.upper() if type(s) == str else s)\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Column Sepssis to Sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename\n",
    "train.rename(columns={\"Sepssis\": \"Sepsis\"}, inplace=True)\n",
    "test.rename(columns={\"Sepssis\": \"Sepsis\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Sepsis to 0 (negative) and 1 (positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "train.loc[train['Sepsis'].isin(['POSITIVE']), 'Sepsis'] = '1'\n",
    "train.loc[train['Sepsis'].isin(['NEGATIVE']), 'Sepsis'] = '0'\n",
    "train['Sepsis'] = train['Sepsis'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_M11_q_low = train[\"M11\"].quantile(0.01)\n",
    "train_M11_q_hi  = train[\"M11\"].quantile(0.99)\n",
    "\n",
    "df_filtered = train[(train[\"M11\"] > train_M11_q_hi) | (train[\"M11\"] < train_M11_q_low) | (train[\"M11\"] == 0)]\n",
    "print(len(df_filtered)/ len(train) * 100)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_M11_q_low = test[\"M11\"].quantile(0.01)\n",
    "test_M11_q_hi  = test[\"M11\"].quantile(0.99)\n",
    "\n",
    "df_filtered = test[(test[\"M11\"] > test_M11_q_hi) | (test[\"M11\"] < test_M11_q_low) | (test[\"M11\"] == 0)]\n",
    "print(len(df_filtered)/ len(test) * 100)\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for checking duplication , outliers and imposible value of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.1 Check duplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "print(\"Number of rows before drop of duplicates  in TRAIN:\", len(train.index))\n",
    "print(\"Number of duplicated records in TRAIN: \", train.duplicated().sum())\n",
    "train.drop_duplicates(inplace=True)\n",
    "print(\"Number of rows after drop of duplicates in TRAIN:\", len(train.index), \"\\n\\n\")\n",
    "\n",
    "\n",
    "# VALIDATION\n",
    "print(\"Number of rows before drop of duplicates in VALIDATION:\", len(test.index))\n",
    "print(\"Number of duplicated records in VALIDATION: \", test.duplicated().sum())\n",
    "validation.drop_duplicates(inplace=True)\n",
    "print(\"Number of rows after drop of duplicates in VALIDATION:\", len(test.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.2 Impossible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have M11 equal and below 0\n",
    "\n",
    "train = train[train[\"M11\"] != 0]\n",
    "\n",
    "test = test[test[\"M11\"] != 0]\n",
    "\n",
    "\n",
    "print(\"TRAIN DATASET: \")\n",
    "print(train[[\"M11\"]].describe().round(2))\n",
    "\n",
    "print(\"TEST DATASET: \")\n",
    "print(test[[\"M11\"]].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "M11( Weight) is impossible with value 0 so I decided drop 0 or below that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 Extra exploration and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.1 Histogram of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "train.hist(figsize=(14,14), xrot=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "PRG ,SK ,TS ,BD2 have the same distribute\n",
    "PL , PR ,M11 have the same distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7.2 Statistic of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\">  OBSERVATION: </span>\n",
    "<li>This train data set has 601 raw and 9 columns.</li>\n",
    "<li>only 34% patient got positive.</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> Outliers detection</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitespace_remover(df):\n",
    "    \"\"\"\n",
    "    The function will remove extra leading and trailing whitespace from the data.\n",
    "    \"\"\"\n",
    "    # iterating over the columns\n",
    "    for i in df.columns:\n",
    "        # checking datatype of each columns\n",
    "        if df[i].dtype == 'object' or df[i].dtype == 'str':\n",
    "            # applying strip function on column\n",
    "            df[i] = df[i].map(str.strip)\n",
    "        else:\n",
    "            # if condition is False then it will do nothing.\n",
    "            pass\n",
    "\n",
    "# remove all the extra whitespace\n",
    "whitespace_remover(train)\n",
    "whitespace_remover(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#1.Create a function that removes outliers\"\"\"\n",
    "def removeOutliers(variable):\n",
    "    \"\"\"Calculates and removes outliers using IQR method.\"\"\"\n",
    "    \n",
    "    # Calculate 1st, 3rd quartiles and iqr.\n",
    "    q1, q3 = variable.quantile(0.25), variable.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # Calculate lower fence and upper fence for outliers\n",
    "    lowerFence, upperFence = q1-1.5*iqr, q3+1.5*iqr   # Any values less than l_fence and greater than u_fence are outliers.\n",
    "    \n",
    "    # Observations that are outliers\n",
    "    outliers = variable[(variable<lowerFence) | (variable>upperFence)]\n",
    "    \n",
    "    # Drop obsevations that are outliers\n",
    "    filtered = variable.drop(outliers.index, axis = 0).reset_index(drop=True)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\"\"\"#2.Create another function to plot boxplot with and without outliers.\"\"\"\n",
    "def plotBoxPlot(variable,filteredVariable):\n",
    "    \"\"\"Plots Box plot of a variable with and without outliers.\n",
    "    We will also use the output of removeOutliers function as the input to this function.\n",
    "    variable = variable with outliers,\n",
    "    filteredVariable = variable without outliers\"\"\"\n",
    "    \n",
    "    # Create subplot object.\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        print_grid=False,\n",
    "    subplot_titles=(f\"{variable.name} Distribution with Outliers\", f\"{variable.name} Distribution without Outliers\"))\n",
    "    \n",
    "    # This trace plots boxplot with outliers\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x = variable,\n",
    "            name = \"\", # This removes trace 0\n",
    "            marker = dict(color=\"darkred\")\n",
    "        ),\n",
    "    row=1,col=1)\n",
    "    \n",
    "    # This trace plots boxplot without outliers\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x = filteredVariable,\n",
    "            name = \"\",\n",
    "            marker = dict(color=\"green\")\n",
    "        ),\n",
    "    row=2,col=1)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.layout.update(\n",
    "        height=800, \n",
    "        width=870,\n",
    "        showlegend=False,\n",
    "        paper_bgcolor=\"rgb(243, 243, 243)\",\n",
    "        plot_bgcolor=\"rgb(243, 243, 243)\"\n",
    "        )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.layout.xaxis2.update(title=f\"<b>{variable.name}</b>\")\n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot Age with and without outliers.\"\"\"\n",
    "plotBoxPlot(train.Age,removeOutliers(train.Age))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age remove outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Plot Weight with and without outliers.\"\"\"\n",
    "plotBoxPlot(train.M11,removeOutliers(train.M11))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's split the train and test data for bivariate analysis since test data has no Survived values. We need our target variable without missing values to conduct the association test with predictor variables.\"\"\"\n",
    "df_train = train.iloc[:599, :]\n",
    "df_test = train.iloc[599:, :]\n",
    "\n",
    "\"\"\"#1.Create a function that creates boxplot between categorical and numerical variables and calculates biserial correlation.\"\"\"\n",
    "def boxplotAndCorrelation(numVariable,catVariable=df_train.Sepsis):\n",
    "    \"\"\"Return boxplot between a categorical and numerical variable. Also calculates biserial correlation.\n",
    "    numVariable = a numerical variable of interest.\"\"\"\n",
    "    # Calculate point biserial correlation and p value\n",
    "    biserialCorr = stats.pointbiserialr(numVariable,catVariable)[0].round(2)\n",
    "    pValue = stats.pointbiserialr(numVariable,catVariable)[1].round(5)\n",
    "    \n",
    "    # Create subplot object.\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=1,\n",
    "        print_grid=False,\n",
    "    )\n",
    "    \n",
    "    # This trace plots boxplot of categorical variable vs numerical variable\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x = catVariable,\n",
    "            y = numVariable,\n",
    "            marker_color=\"lightseagreen\",\n",
    "            ))\n",
    "    \n",
    "    \n",
    "    # Update layout\n",
    "    fig.layout.update(\n",
    "        height=500, \n",
    "        width=900,\n",
    "        showlegend=False,\n",
    "        title_text= f\"Association between {catVariable.name} and {numVariable.name} (corr: {biserialCorr}, p: {pValue})\",\n",
    "        paper_bgcolor=\"rgb(243, 243, 243)\",\n",
    "        plot_bgcolor=\"rgb(243, 243, 243)\"\n",
    "        )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.layout.xaxis1.update(title=f\"<b>{catVariable.name}</b>\")\n",
    "    fig.layout.yaxis1.update(title=f\"<b>{numVariable.name}</b>\")\n",
    "    return fig.show()\n",
    "\n",
    "\n",
    "\"\"\"#2.Create another function to calculate mean when grouped by categorical variable. And also plot the grouped mean.\"\"\"\n",
    "def numGroupedByCat(numVariable,catVariable=df_train.Sepsis):\n",
    "    \"\"\"Returns a barplot showing mean of numerical variable across the class of categorical variable.\"\"\"\n",
    "    \n",
    "    # Calculates mean across different classes of categorical variable\n",
    "    numGroupedByCat = numVariable.groupby(catVariable).mean().round(2)\n",
    "    \n",
    "    # Create subplot object.\n",
    "    fig = make_subplots(\n",
    "        rows=1,\n",
    "        cols=1,\n",
    "        print_grid=False,\n",
    "    )\n",
    "    \n",
    "    # This trace plots barplot\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x = numGroupedByCat.index,\n",
    "            y = numGroupedByCat,\n",
    "            text=numGroupedByCat,\n",
    "            hoverinfo=\"x+y\",\n",
    "            textposition=\"auto\",\n",
    "            textfont=dict(family=\"sans serif\",size=15)\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.layout.update(\n",
    "        height=500, \n",
    "        width=900,\n",
    "        showlegend=False,\n",
    "        title_text= f\"Mean {numVariable.name} across {catVariable.name}\",\n",
    "        paper_bgcolor=\"rgb(243, 243, 243)\",\n",
    "        plot_bgcolor=\"rgb(243, 243, 243)\"\n",
    "        )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.layout.xaxis1.update(title=f\"<b>{catVariable.name}</b>\")\n",
    "    fig.layout.yaxis1.update(title=f\"<b>Mean {numVariable.name}</b>\")\n",
    "    return fig.show()\n",
    "\n",
    "    \n",
    "\"\"\"#3.This function plots histogram of numerical variable for every class of categorical variable.\"\"\"\n",
    "def numHistByCat(numVariable,catVariable=df_train.Sepsis):\n",
    "    \"\"\"Returns numerical variable distribution across classes of categorical variable.\"\"\"\n",
    "    fig,ax = plt.subplots(1,1,figsize = (18,7))\n",
    "    font_size = 15\n",
    "    title_size = 18\n",
    "    numVariable[catVariable==1].hist(bins=50,color=\"green\", label = \"survived\", grid = False, alpha=0.5)\n",
    "    numVariable[catVariable==0].hist(bins=50,color=\"red\", label = \"died\", grid = False, alpha=0.5)\n",
    "    ax.set_yticks([])\n",
    "    ax.tick_params(axis=\"x\", labelsize=font_size)\n",
    "    ax.set_xlabel(f\"{numVariable.name}\", fontsize = font_size)\n",
    "    ax.set_title(f\"{numVariable.name} Distribution of Survivors vs Victims\", fontsize = title_size)\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "\n",
    "   \n",
    "\"\"\"#4.Create a function to calculate anova between numerical and categorical variable.\"\"\"\n",
    "def calculateAnova(numVariable, catVariable=df_train.Sepsis):\n",
    "    \"\"\"Returns f statistics and p value after anova calculation.\"\"\"\n",
    "    \n",
    "    groupNumVariableByCatVariable1 = numVariable[catVariable==1] # Group our numerical variable by categorical variable(1). Group Fair by survivors\n",
    "    groupNumVariableByCatVariable0 = numVariable[catVariable==0] # Group our numerical variable by categorical variable(0). Group Fare by victims\n",
    "    # Calculate one way anova\n",
    "    fValue, pValue = stats.f_oneway(groupNumVariableByCatVariable1, groupNumVariableByCatVariable0) # Calculate f statistics and p value\n",
    "    return f\"Anova Result between {numVariable.name} & {catVariable.name}: f=> {fValue}, p=> {pValue}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a boxplot to visualize the strength of association of Sepsis with Age. Also calculate biserial correlation.\"\"\"\n",
    "boxplotAndCorrelation(df_train.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"So the mean fare of survivors should be much more (from positive correlation or boxplot interpretation) than those who died. Calculate mean fare paid by the survivors as well as by the victims.\"\"\"\n",
    "numGroupedByCat(df_train.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot histogram of survivor's vs victims fare.\"\"\"\n",
    "numHistByCat(df_train.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's perform ANOVA between Fare and Survived. One can omit this step. I perform just to show how anova is performed if there were more than two groups in our categorical variable.\"\"\"\n",
    "calculateAnova(df_train.Age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a boxplot to visualize the strength of association of Survived with Fare. Also calculate biserial correlation.\"\"\"\n",
    "boxplotAndCorrelation(df_train.M11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"So the mean fare of survivors should be much more (from positive correlation or boxplot interpretation) than those who died. Calculate mean fare paid by the survivors as well as by the victims.\"\"\"\n",
    "numGroupedByCat(df_train.M11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot histogram of survivor's vs victims fare.\"\"\"\n",
    "numHistByCat(df_train.M11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Let's perform ANOVA between Fare and Survived. One can omit this step. I perform just to show how anova is performed if there were more than two groups in our categorical variable.\"\"\"\n",
    "calculateAnova(df_train.M11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percent of people having sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='Age',\n",
    "            kind=\"violin\", data=train)\n",
    "plt.title('How old the people having sepsis are?', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"Age\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓<span style=\"color:blue\">  OBSERVATION: </span>\n",
    "Observe from the plot , patients get sepsis mostly more than 30\n",
    "Begin from 31 the positive tends to dominance negative which means people > 30 get sepsis more than young people\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight vs Sepsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='M11',\n",
    "            kind=\"box\", data=train)\n",
    "plt.title('Weight vs Sepsis', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "People are obese likely get sepsis\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30));\n",
    "sns.set(style=\"whitegrid\");\n",
    "sns.catplot(x='Sepsis', y='PL',\n",
    "            kind=\"box\", data=train);\n",
    "plt.title('PL vs Sepsis', fontsize = 20, pad = 30);\n",
    "plt.ylabel(\"PL\");\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='SK',\n",
    "            kind=\"box\", data=train)\n",
    "plt.title('SK vs Sepsis', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"SK\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='TS',\n",
    "            kind=\"box\", data=train)\n",
    "plt.title('TS vs Sepsis', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"TS\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(40,30))\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.catplot(x='Sepsis', y='BD2',\n",
    "            kind=\"box\", data=train)\n",
    "plt.title('BD2 vs Sepsis', fontsize = 20, pad = 30)\n",
    "plt.ylabel(\"BD2\")\n",
    "plt.xlabel(\"Sepsis\", fontsize = 15, labelpad = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_impute(train, test):\n",
    "    for i in [train, test]:\n",
    "        i['Age_Null_Flag'] = i['Age'].apply(lambda x: 1 if pd.isnull(x) else 0)\n",
    "        data = train.groupby(['Name_Title', 'Pclass'])['Age']\n",
    "        i['Age'] = data.transform(lambda x: x.fillna(x.mean()))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10,7.5]\n",
    "sns.boxplot(data=train, orient=\"v\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 style=\"color:red;font-size:40px;font-family:Georgia;text-align:center;\"><strong>  3. Feature Engineering </strong></h1>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total missing values in TRAIN:\", train.isna().sum().sum())\n",
    "print(\"Total missing values in TEST:\", test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "No missing values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_3_chart(df, feature):\n",
    "    ## Importing seaborn, matplotlab and scipy modules. \n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    from scipy import stats\n",
    "    import matplotlib.style as style\n",
    "    style.use('fivethirtyeight')\n",
    "\n",
    "    ## Creating a customized chart. and giving in figsize and everything. \n",
    "    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n",
    "    ## creating a grid of 3 cols and 3 rows. \n",
    "    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n",
    "    #gs = fig3.add_gridspec(3, 3)\n",
    "\n",
    "    ## Customizing the histogram grid. \n",
    "    ax1 = fig.add_subplot(grid[0, :2])\n",
    "    ## Set the title. \n",
    "    ax1.set_title('Histogram')\n",
    "    ## plot the histogram. \n",
    "    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n",
    "\n",
    "    # customizing the QQ_plot. \n",
    "    ax2 = fig.add_subplot(grid[1, :2])\n",
    "    ## Set the title. \n",
    "    ax2.set_title('QQ_plot')\n",
    "    ## Plotting the QQ_Plot. \n",
    "    stats.probplot(df.loc[:,feature], plot = ax2)\n",
    "\n",
    "    ## Customizing the Box Plot. \n",
    "    ax3 = fig.add_subplot(grid[:, 2])\n",
    "    ## Set title. \n",
    "    ax3.set_title('Box Plot')\n",
    "    ## Plotting the box plot. \n",
    "    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n",
    "    \n",
    "plotting_3_chart(train, 'Sepsis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "> There are lots of outliners\n",
    "\n",
    "> Sepsis is normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skewness and kurtosis\n",
    "print(\"Skewness: \" + str(train['Sepsis'].skew()))\n",
    "print(\"Kurtosis: \" + str(train['Sepsis'].kurt()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "\n",
    "Positive Skewess\n",
    "> There are more positive cases than average but at low \n",
    "\n",
    "Negative Kurtosis\n",
    ">This simply means that more sepsis data values are located near the mean and less data values are located on the tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew  # for some statistics\n",
    "numeric_feats = train.dtypes[train.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "print(\"\\nSkew in numerical features: \\n\")\n",
    "skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "skewness.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "### 3.2 [Box Cox Transformation of (highly) skewed features](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.special.boxcox1p.html)\n",
    "+ We use the scipy function boxcow which computes the Box-Cox transformation of  1+x .\n",
    "\n",
    "+ Note that setting  λ=0  is equivalent to log1p used above for the target variable.\n",
    "\n",
    "+ See this page for more details on Box Cox Transformation as well as the scipy function's page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the correlation of all the features with target variable. \n",
    "(train.corr()**2)[\"Sepsis\"].sort_values(ascending = False)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "PL is the most correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a>\n",
    "### 3.3 Assumptions of Regression\n",
    "\n",
    "* **Linearity ( Correct functional form )** \n",
    "* **Homoscedasticity ( Constant Error Variance )( vs Heteroscedasticity )**\n",
    "* **Independence of Errors ( vs Autocorrelation )**\n",
    "* **Multivariate Normality ( Normality of Errors )**\n",
    "* **No or little Multicollinearity** \n",
    "\n",
    "> So, **How do I check regression assumptions? I fit a regression line and look for the variability of the response data along the regression line.** Let's apply this to each one of them.\n",
    "\n",
    "> **Linearity(Correct functional form):** \n",
    "Linear regression needs the relationship between each independent variable and the dependent variable to be linear. The linearity assumption can be tested with scatter plots. The following two examples depict two cases, where no or little linearity is present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing multicollinary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot sizing. \n",
    "fig, (ax1, ax2) = plt.subplots(figsize = (12,8), ncols=2,sharey=False)\n",
    "## Scatter plotting for Severity and Distance(mi).\n",
    "sns.scatterplot( x = train['M11'], y = train.Sepsis,  ax=ax1)\n",
    "## Putting a regression line. \n",
    "sns.regplot(x=train['M11'], y=train.Sepsis, ax=ax1)\n",
    "\n",
    "## Scatter plotting for Severity and ['Wind_Speed(mph)'].\n",
    "sns.scatterplot(x = train['PL'],y = train.Sepsis, ax=ax2, color='pink')\n",
    "## regression line for MasVnrArea and Severity.\n",
    "sns.regplot(x=train['PL'], y=train.Sepsis, ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotting_3_chart(train, 'Sepsis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "No feature have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "sns.distplot(train['Sepsis'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['Sepsis'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Sepsis distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['Sepsis'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the log transformation removes the normality of errors, which solves most of the other errors we talked about above. Let's make a comparison of the pre-transformed and post-transformed state of residual plots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, we see that the pre-transformed chart on the left has heteroscedasticity, and the post-transformed chart on the right has Homoscedasticity(almost an equal amount of variance across the zero lines). It looks like a blob of data points and doesn't seem to give away any relationships. That's the sort of relationship we would like to see to avoid some of these assumptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Correlation\n",
    "\n",
    "As we examined these scatter plots, I thought it was time to explain the Multiple Linear Regression assumptions. Before constructing a multiple linear regression model, we must ensure that the following assumptions are correct.\n",
    "\n",
    "Already, we can observe some potentially intriguing correlations between the objective variable (the number of fatal accidents) and the feature variables (the remaining three columns).\n",
    "\n",
    "The Pearson correlation coefficient matrix may be used to quantify the pairwise associations shown in the scatter plots. The Pearson correlation coefficient is one of the most often used methods for quantifying correlation between variables, and the following criteria are frequently employed by convention:\n",
    "\n",
    "0.2 = weak\n",
    "0.5 = medium\n",
    "0.8 = strong\n",
    "0.9 = very strong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare sepsis level rate across numerical columns\n",
    "pd.pivot_table(train, index = 'Sepsis', values = ['PRG', 'PL', 'PR', 'SK', 'TS', 'M11', 'BD2', 'Age', 'Insurance', 'Sepsis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "> Multiple value for different column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M11 filter Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter m11 using groupby\n",
    "train.loc[(train[\"M11\"] == 0), 'M11'] = np.nan\n",
    "train['M11'] = train.groupby('Age')['M11'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter SK using groupby\n",
    "train.loc[(train[\"SK\"] >90), 'SK'] = np.nan\n",
    "train['SK'] = train.groupby('Age')['SK'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter pl using groupby\n",
    "train.loc[(train[\"PL\"] == 0), 'PL'] = np.nan\n",
    "train['PL'] = train.groupby('Age')['PL'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter SK using groupby\n",
    "train.loc[(train[\"TS\"] >650), 'TS'] = np.nan\n",
    "train['TS'] = train.groupby('Age')['TS'].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove PR with 0 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[(test[\"PR\"] == 0), 'PR'] = test[\"PR\"].mean()\n",
    "train.loc[(train[\"PR\"] == 0), 'PR'] = train[\"PR\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove M11 with 0 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[(test[\"M11\"] == 0), 'PR'] = test[\"M11\"].mean()\n",
    "train.loc[(train[\"M11\"] == 0), 'PR'] = train[\"M11\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection \n",
    "from collections import Counter\n",
    "\n",
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    \n",
    "    return multiple_outliers   \n",
    "\n",
    "# detect outliers from Age, SibSp , Parch and Fare\n",
    "Outliers_to_drop = detect_outliers(train,2,[\"Age\",\"M11\",\"BD2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[Outliers_to_drop] # Show the outliers rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No outliner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##  ❓<span style=\"color:blue \">OBSERVATION: </span>\n",
    "- There is no missing value\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3. Check corelation for dropping\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corelation = train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.style as style\n",
    "import numpy as np\n",
    "mask = np.zeros_like(train.corr(), dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.set_style('darkgrid')\n",
    "plt.subplots(figsize = (15,12))\n",
    "sns.heatmap(train.corr(), \n",
    "            annot=True,\n",
    "            mask = mask,\n",
    "            cmap = 'RdBu', ## in order to reverse the bar replace \"RdBu\" with \"RdBu_r\"\n",
    "            linewidths=.9, \n",
    "            linecolor='red',\n",
    "            fmt='.2g',\n",
    "            center = 0,\n",
    "            square=True)\n",
    "plt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20, pad = 40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓ <span style=\"color:blue \"> OBSERVATION : </span>\n",
    " All corelation is under 50% so all data is fine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (train.info())\n",
    "print (\"*\"*40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓<span style=\"color:blue\"> OBSERVATION: </span>\n",
    "<li>We may dont have missing values in our features.</Li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Rename the copy DataFrame to fit into stats model\n",
    "data1 = train.rename(columns={'PRG': 'PRG', 'PL': 'PL', 'PR': 'PR', 'SK': 'SK', 'TS': 'TS', 'M11': 'M11', 'BD2': 'BD2', 'Age': 'Age'})\n",
    "\n",
    "formula = 'Sepsis ~ '+ '+'.join(data1.columns[1:])\n",
    "\n",
    "model = ols(formula='Sepsis ~ PRG+PL+PR+SK+TS+M11+BD2+Age', data=data1).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of the Model Coefficient, the P-value, the R-squared\n",
    "> The output above shows that, when the other variables remain constant, if we compare two applicants whose 'M11' differ by one unit, the applicant with higher 'M11' will, on average, have 0.011 units higher 'Income'.\n",
    "> Using the P>|t| result, I can infer that the variables all independent variables are the statistically significant variables, as their p-value is less than 0.8.\n",
    "> The Adj. R-squared 0.408 indicates the amount of variability not being explained by my model that much\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train - Test - Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Drop column ID and Insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(columns=['Insurance'])\n",
    "test = test.drop(columns=['Insurance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.drop(columns=['PRG'])\n",
    "# test = test.drop(columns=['PRG'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['Sepsis'], axis = 1)\n",
    "y = train[\"Sepsis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1 style=\"color:red;font-size:40px;font-family:Georgia;text-align:center;\"><strong>3. Model Traning </strong></h1>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "seed = 42\n",
    "X_train , X_test ,y_train ,y_test = train_test_split(X,y , train_size = 0.7 , test_size = 0.3 , random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42, sampling_strategy=1.0)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "st_scale = RobustScaler()\n",
    "\n",
    "X_train = st_scale.fit_transform(X_train)\n",
    "X_test = st_scale.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"See the dimensions of input and output data set.\"\"\"\n",
    "print(f\"Input Matrix Dimension: {X_train.shape}\")\n",
    "print(f\"Output Vector Dimension: {y_train.shape}\")\n",
    "print(f\"Test Data Dimension: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now initialize all the classifiers object.\"\"\"\n",
    "\"\"\"#1.Logistic Regression\"\"\"\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\"\"\"#3.Random Forest Classifier\"\"\"\n",
    "rf = RandomForestClassifier(random_state = seed, n_estimators = 100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#6.Decision Tree Classifier\"\"\"\n",
    "dt = DecisionTreeClassifier(random_state = seed)\n",
    "dt.fit(X_train,y_train)\n",
    "y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\"\"\"For DT, the following hyperparameters are usually tunned.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def plot_confusionmatrix(y_train_pred,y_train,dom):\n",
    "    print(f'{dom} Confusion matrix')\n",
    "    cf = confusion_matrix(y_train_pred,y_train)\n",
    "    sns.heatmap(cf,annot=True,yticklabels=['Positive','Negative']\n",
    "               ,xticklabels=['Positive','Negative'],cmap='Blues', fmt='g')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = dt.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "print(ccp_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each alpha we will append our model to a list\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = clfs[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "plt.scatter(ccp_alphas,node_counts)\n",
    "plt.scatter(ccp_alphas,depth)\n",
    "plt.plot(ccp_alphas,node_counts,label='no of nodes',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,depth,label='depth',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "for c in clfs:\n",
    "    y_train_pred = c.predict(X_train)\n",
    "    y_test_pred = c.predict(X_test)\n",
    "    train_acc.append(f1_score(y_train_pred,y_train))\n",
    "    test_acc.append(f1_score(y_test_pred,y_test))\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.scatter(ccp_alphas,train_acc)\n",
    "plt.scatter(ccp_alphas,test_acc)\n",
    "plt.plot(ccp_alphas,train_acc,label='train_f1',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,test_acc,label='test_f1',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ = DecisionTreeClassifier(random_state=42,ccp_alpha=0.025)\n",
    "clf_.fit(X_train,y_train)\n",
    "y_train_pred = clf_.predict(X_train)\n",
    "y_test_pred = clf_.predict(X_test)\n",
    "\n",
    "plot_confusionmatrix(y_train_pred,y_train,dom='Train')\n",
    "plot_confusionmatrix(y_test_pred,y_test,dom='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = f1_score(y_train, y_train_pred)\n",
    "test_acc = f1_score(y_test, y_test_pred)\n",
    "print(\"Train F1 score: \" + str(train_acc))\n",
    "print(\"Test F1 score: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtparams = {'max_depth': [2,4,6,8,10,20,None],\n",
    "         'min_samples_split': [2,3,4],\n",
    "         'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "           'criterion': [\"gini\", \"entropy\"]}\n",
    "cv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n",
    "gcv = GridSearchCV(estimator=dt,param_grid=dtparams,cv=cv, n_jobs=-1, verbose=1, scoring = \"f1\")\n",
    "gcv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the best of everything. \n",
    "print (gcv.best_score_)\n",
    "print (gcv.best_params_)\n",
    "print(gcv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "max_depths = np.linspace(2, 20, 20, endpoint=True) # List of values for tuning\n",
    "train_results = [] # Store train accuracy results\n",
    "test_results = []  # Store test accuracy results\n",
    "for max_depth in max_depths:\n",
    "#    decisionTree =DecisionTreeClassifier(criterion = 'gini',\n",
    "#                                             max_depth = max_depth,\n",
    "#                                               max_features='auto',\n",
    "#                                               ccp_alpha = 0.024, \n",
    "#                                               min_samples_leaf=5, \n",
    "#                                               min_samples_split=2,\n",
    "#                                                random_state=42,\n",
    "#                                                 splitter = \"best\")\n",
    "   decisionTree =DecisionTreeClassifier(criterion = 'entropy',\n",
    "                                        max_depth = 2,\n",
    "                                              ccp_alpha = 0.025,\n",
    "                                        min_samples_leaf= 10,\n",
    "                                        min_samples_split=2,\n",
    "                                               random_state=42,\n",
    "                                                splitter = \"best\")\n",
    "   decisionTree.fit(X_train, y_train)\n",
    "   train_pred = decisionTree.predict(X_train)\n",
    "   train_acc = f1_score(y_train, train_pred)\n",
    "   # Add accuracy score to previous train results\n",
    "   train_results.append(train_acc)\n",
    "\n",
    "   #test\n",
    "   test_pred = decisionTree.predict(X_test)\n",
    "   test_acc = f1_score(y_test, test_pred)\n",
    "   # Add auc score to previous test results\n",
    "   test_results.append(test_acc)\n",
    "\n",
    "   print('The Training f1 Accuracy for max_depth {} is:'.format(max_depth), train_acc)\n",
    "   print('The Test f1 Accuracy for max_depth {} is:'.format(max_depth), test_acc)\n",
    "\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, \"b\", label='Train AUC')\n",
    "line2, = plt.plot(max_depths, test_results, \"r\", label='Test AUC')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = f1_score(y_train, train_pred)\n",
    "test_acc = f1_score(y_test, test_pred)\n",
    "print(\"Train F1 score: \" + str(train_acc))\n",
    "print(\"Test F1 score: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train score {accuracy_score(train_pred,y_train)}')\n",
    "print(f'Test score {accuracy_score(test_pred,y_test)}')\n",
    "plot_confusionmatrix(train_pred,y_train,dom='Train')\n",
    "plot_confusionmatrix(test_pred,y_test,dom='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a function that returns learning curves for different classifiers.\"\"\"\n",
    "def plotLearningCurve(model):\n",
    "    \"\"\"Returns a plot of learning curve of a model.\"\"\"\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X, y = X_train, y_train\n",
    "    # Create CV training and test scores for various training set sizes\n",
    "    trainSizes, trainScores, testScores = learning_curve(model, X, y, cv = 10,\n",
    "                                                    scoring=\"roc_auc\", n_jobs = -1, \n",
    "                                                    train_sizes = np.linspace(0.01, 1.0, 17), # 17 different sizes of the training set\n",
    "                                                    random_state = seed)\n",
    "                                                    \n",
    "\n",
    "    # Create means and standard deviations of training set scores\n",
    "    trainMean = np.mean(trainScores, axis = 1)\n",
    "    trainStd = np.std(trainScores, axis = 1)\n",
    "\n",
    "    # Create means and standard deviations of test set scores\n",
    "    testMean = np.mean(testScores, axis = 1)\n",
    "    testStd = np.std(testScores, axis = 1)\n",
    "\n",
    "    # Draw lines\n",
    "    plt.plot(trainSizes, trainMean, \"o-\", color = \"red\",  label = \"training score\")\n",
    "    plt.plot(trainSizes, testMean, \"o-\", color = \"green\", label = \"cross-validation score\")\n",
    "    \n",
    "    # Draw bands\n",
    "    plt.fill_between(trainSizes, trainMean - trainStd, trainMean + trainStd, alpha = 0.1, color = \"r\") # Alpha controls band transparency.\n",
    "    plt.fill_between(trainSizes, testMean - testStd, testMean + testStd, alpha = 0.1, color = \"g\")\n",
    "\n",
    "    # Create plot\n",
    "    font_size = 15\n",
    "    plt.xlabel(\"Training Set Size\", fontsize = font_size)\n",
    "    plt.ylabel(\"Accuracy Score\", fontsize = font_size)\n",
    "    plt.xticks(fontsize = font_size)\n",
    "    plt.yticks(fontsize = font_size)\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now plot learning curves of the optimized models in subplots.\"\"\"\n",
    "plt.figure(figsize = (25,25))\n",
    "lcModels = [dt]\n",
    "lcLabels = [\"DT\"]\n",
    "\n",
    "for ax, model, label in zip (range(1,9), lcModels, lcLabels):\n",
    "    plt.subplot(4,2,ax)\n",
    "    plotLearningCurve(model)\n",
    "    plt.title(label, fontsize = 18)\n",
    "plt.suptitle(\"Learning Curves of Optimized Models\", fontsize = 28)\n",
    "plt.tight_layout(rect = [0, 0.03, 1, 0.97])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#6.Decision Tree Classifier\"\"\"\n",
    "lr = LogisticRegression(random_state = seed)\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\"\"\"For DT, the following hyperparameters are usually tunned.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def plot_confusionmatrix(y_train_pred,y_train,dom):\n",
    "    print(f'{dom} Confusion matrix')\n",
    "    cf = confusion_matrix(y_train_pred,y_train)\n",
    "    sns.heatmap(cf,annot=True,yticklabels=['Positive','Negative']\n",
    "               ,xticklabels=['Positive','Negative'],cmap='Blues', fmt='g')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrparams = {\"penalty\":[\"l1\", \"l2\"],\n",
    "            \"C\": np.logspace(0, 4, 10),\n",
    "            \"max_iter\":[5000]}\n",
    "cv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n",
    "gcv = GridSearchCV(estimator=lr,param_grid=lrparams,cv=cv)\n",
    "gcv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the best of everything. \n",
    "print (gcv.best_score_)\n",
    "print (gcv.best_params_)\n",
    "print(gcv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "max_depths = np.linspace(1, 2, 2, endpoint=True) # List of values for tuning\n",
    "train_results = [] # Store train accuracy results\n",
    "test_results = []  # Store test accuracy results\n",
    "for max_depth in max_depths:\n",
    "   decisionTree = LogisticRegression(C= 7.742636826811269, \n",
    "                                          max_iter= 5000, \n",
    "                                          penalty= 'l2')\n",
    "   decisionTree.fit(X_train, y_train)\n",
    "   train_pred = decisionTree.predict(X_train)\n",
    "   train_acc = f1_score(y_train, train_pred)\n",
    "   # Add accuracy score to previous train results\n",
    "   train_results.append(train_acc)\n",
    "\n",
    "   #test\n",
    "   test_pred = decisionTree.predict(X_test)\n",
    "   test_acc = f1_score(y_test, test_pred)\n",
    "   # Add auc score to previous test results\n",
    "   test_results.append(test_acc)\n",
    "\n",
    "   print('The Training f1 Accuracy for max_depth {} is:'.format(max_depth), train_acc)\n",
    "   print('The Test f1 Accuracy for max_depth {} is:'.format(max_depth), test_acc)\n",
    "\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, \"b\", label='Train AUC')\n",
    "line2, = plt.plot(max_depths, test_results, \"r\", label='Test AUC')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = f1_score(y_train, train_pred)\n",
    "test_acc = f1_score(y_test, test_pred)\n",
    "print(\"Train F1 score: \" + str(train_acc))\n",
    "print(\"Test F1 score: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train score {accuracy_score(train_pred,y_train)}')\n",
    "print(f'Test score {accuracy_score(test_pred,y_test)}')\n",
    "plot_confusionmatrix(train_pred,y_train,dom='Train')\n",
    "plot_confusionmatrix(test_pred,y_test,dom='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a function that returns learning curves for different classifiers.\"\"\"\n",
    "def plotLearningCurve(model):\n",
    "    \"\"\"Returns a plot of learning curve of a model.\"\"\"\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X, y = X_train, y_train\n",
    "    # Create CV training and test scores for various training set sizes\n",
    "    trainSizes, trainScores, testScores = learning_curve(model, X, y, cv = 10,\n",
    "                                                    scoring=\"accuracy\", n_jobs = -1, \n",
    "                                                    train_sizes = np.linspace(0.01, 1.0, 17), # 17 different sizes of the training set\n",
    "                                                    random_state = seed)\n",
    "                                                    \n",
    "\n",
    "    # Create means and standard deviations of training set scores\n",
    "    trainMean = np.mean(trainScores, axis = 1)\n",
    "    trainStd = np.std(trainScores, axis = 1)\n",
    "\n",
    "    # Create means and standard deviations of test set scores\n",
    "    testMean = np.mean(testScores, axis = 1)\n",
    "    testStd = np.std(testScores, axis = 1)\n",
    "\n",
    "    # Draw lines\n",
    "    plt.plot(trainSizes, trainMean, \"o-\", color = \"red\",  label = \"training score\")\n",
    "    plt.plot(trainSizes, testMean, \"o-\", color = \"green\", label = \"cross-validation score\")\n",
    "    \n",
    "    # Draw bands\n",
    "    plt.fill_between(trainSizes, trainMean - trainStd, trainMean + trainStd, alpha = 0.1, color = \"r\") # Alpha controls band transparency.\n",
    "    plt.fill_between(trainSizes, testMean - testStd, testMean + testStd, alpha = 0.1, color = \"g\")\n",
    "\n",
    "    # Create plot\n",
    "    font_size = 15\n",
    "    plt.xlabel(\"Training Set Size\", fontsize = font_size)\n",
    "    plt.ylabel(\"Accuracy Score\", fontsize = font_size)\n",
    "    plt.xticks(fontsize = font_size)\n",
    "    plt.yticks(fontsize = font_size)\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now plot learning curves of the optimized models in subplots.\"\"\"\n",
    "plt.figure(figsize = (25,25))\n",
    "lcModels = [dt]\n",
    "lcLabels = [\"DT\"]\n",
    "\n",
    "for ax, model, label in zip (range(1,9), lcModels, lcLabels):\n",
    "    plt.subplot(4,2,ax)\n",
    "    plotLearningCurve(model)\n",
    "    plt.title(label, fontsize = 18)\n",
    "plt.suptitle(\"Learning Curves of Optimized Models\", fontsize = 28)\n",
    "plt.tight_layout(rect = [0, 0.03, 1, 0.97])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raindom Forest\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#6.Decision Tree Classifier\"\"\"\n",
    "rf = RandomForestClassifier(random_state = seed)\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\"\"\"For DT, the following hyperparameters are usually tunned.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def plot_confusionmatrix(y_train_pred,y_train,dom):\n",
    "    print(f'{dom} Confusion matrix')\n",
    "    cf = confusion_matrix(y_train_pred,y_train)\n",
    "    sns.heatmap(cf,annot=True,yticklabels=['Positive','Negative']\n",
    "               ,xticklabels=['Positive','Negative'],cmap='Blues', fmt='g')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "random_search = {'criterion': ['entropy', 'gini'],\n",
    "               'max_depth': list(np.linspace(10, 1200, 10, dtype = int)) + [None],\n",
    "               'max_features': ['auto', 'sqrt','log2', None],\n",
    "               'min_samples_leaf': [4, 6, 8, 12],\n",
    "               'min_samples_split': [5, 7, 10, 14],\n",
    "               'n_estimators': list(np.linspace(151, 1200, 10, dtype = int))}\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "model = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 80, \n",
    "                               cv = 4, verbose= 5, random_state= 101, n_jobs = -1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the best of everything. \n",
    "print (model.best_score_)\n",
    "print (model.best_params_)\n",
    "print(model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "max_depths = np.linspace(1, 2, 2, endpoint=True) # List of values for tuning\n",
    "train_results = [] # Store train accuracy results\n",
    "test_results = []  # Store test accuracy results\n",
    "for max_depth in max_depths:\n",
    "   rf = RandomForestClassifier(n_estimators= 1200,\n",
    "                                         min_samples_split= 10,\n",
    "                                         min_samples_leaf= 2,\n",
    "                                         max_features= 'auto',\n",
    "                                         max_depth= 10,\n",
    "                                         criterion= 'gini')\n",
    "\n",
    "   rf.fit(X_train, y_train)\n",
    "   train_pred = rf.predict(X_train)\n",
    "   train_acc = f1_score(y_train, train_pred)\n",
    "   # Add accuracy score to previous train results\n",
    "   train_results.append(train_acc)\n",
    "\n",
    "   #test\n",
    "   test_pred = rf.predict(X_test)\n",
    "   test_acc = f1_score(y_test, test_pred)\n",
    "   # Add auc score to previous test results\n",
    "   test_results.append(test_acc)\n",
    "\n",
    "   print('The Training f1 Accuracy for max_depth {} is:'.format(max_depth), train_acc)\n",
    "   print('The Test f1 Accuracy for max_depth {} is:'.format(max_depth), test_acc)\n",
    "\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, \"b\", label='Train AUC')\n",
    "line2, = plt.plot(max_depths, test_results, \"r\", label='Test AUC')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = f1_score(y_train, train_pred)\n",
    "test_acc = f1_score(y_test, test_pred)\n",
    "print(\"Train F1 score: \" + str(train_acc))\n",
    "print(\"Test F1 score: \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train score {accuracy_score(train_pred,y_train)}')\n",
    "print(f'Test score {accuracy_score(test_pred,y_test)}')\n",
    "plot_confusionmatrix(train_pred,y_train,dom='Train')\n",
    "plot_confusionmatrix(test_pred,y_test,dom='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a function that returns learning curves for different classifiers.\"\"\"\n",
    "def plotLearningCurve(model):\n",
    "    \"\"\"Returns a plot of learning curve of a model.\"\"\"\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X, y = X_train, y_train\n",
    "    # Create CV training and test scores for various training set sizes\n",
    "    trainSizes, trainScores, testScores = learning_curve(model, X, y, cv = 10,\n",
    "                                                    scoring=\"accuracy\", n_jobs = -1, \n",
    "                                                    train_sizes = np.linspace(0.01, 1.0, 17), # 17 different sizes of the training set\n",
    "                                                    random_state = seed)\n",
    "                                                    \n",
    "\n",
    "    # Create means and standard deviations of training set scores\n",
    "    trainMean = np.mean(trainScores, axis = 1)\n",
    "    trainStd = np.std(trainScores, axis = 1)\n",
    "\n",
    "    # Create means and standard deviations of test set scores\n",
    "    testMean = np.mean(testScores, axis = 1)\n",
    "    testStd = np.std(testScores, axis = 1)\n",
    "\n",
    "    # Draw lines\n",
    "    plt.plot(trainSizes, trainMean, \"o-\", color = \"red\",  label = \"training score\")\n",
    "    plt.plot(trainSizes, testMean, \"o-\", color = \"green\", label = \"cross-validation score\")\n",
    "    \n",
    "    # Draw bands\n",
    "    plt.fill_between(trainSizes, trainMean - trainStd, trainMean + trainStd, alpha = 0.1, color = \"r\") # Alpha controls band transparency.\n",
    "    plt.fill_between(trainSizes, testMean - testStd, testMean + testStd, alpha = 0.1, color = \"g\")\n",
    "\n",
    "    # Create plot\n",
    "    font_size = 15\n",
    "    plt.xlabel(\"Training Set Size\", fontsize = font_size)\n",
    "    plt.ylabel(\"Accuracy Score\", fontsize = font_size)\n",
    "    plt.xticks(fontsize = font_size)\n",
    "    plt.yticks(fontsize = font_size)\n",
    "    plt.legend(loc = \"best\")\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now plot learning curves of the optimized models in subplots.\"\"\"\n",
    "plt.figure(figsize = (25,25))\n",
    "lcModels = [dt]\n",
    "lcLabels = [\"DT\"]\n",
    "\n",
    "for ax, model, label in zip (range(1,9), lcModels, lcLabels):\n",
    "    plt.subplot(4,2,ax)\n",
    "    plotLearningCurve(model)\n",
    "    plt.title(label, fontsize = 18)\n",
    "plt.suptitle(\"Learning Curves of Optimized Models\", fontsize = 28)\n",
    "plt.tight_layout(rect = [0, 0.03, 1, 0.97])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
